{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import firwin\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "import time\n",
    "import scipy.signal as signal\n",
    "\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_samples, block_size, use_complex):\n",
    "    train_data = torch.randint(2, (num_samples, block_size)).float()\n",
    "    test_data = torch.randint(2, (2500, block_size)).float()\n",
    "    if use_complex:\n",
    "        train_zeros = torch.zeros(num_samples, block_size*2).float()\n",
    "        test_zeros = torch.zeros(2500, block_size*2).float()\n",
    "        train_zeros[:, :block_size] = train_data\n",
    "        test_zeros[:, :block_size] = test_data\n",
    "        train_data = train_zeros\n",
    "        test_data = test_zeros\n",
    "\n",
    "    train_labels = train_data\n",
    "    test_labels = test_data\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data, labels):\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        X = self.data[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, snr, num_taps):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.snr = snr\n",
    "        self.num_taps = num_taps\n",
    "        \n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.filter_hidden_dim = num_taps + hidden_dim - 1\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_size)\n",
    "    \n",
    "    def awgn(self, x, batch_idx, discrete_jumps, num_samples):\n",
    "        # Assumes batch size is 1, noise is delta function at frequency proportional to size of dataset.\n",
    "        # So one epoch passes through all delta frequencies.\n",
    "        L = np.linspace(0, np.pi, num=discrete_jumps)\n",
    "        snr_lin = 10**(0.1*self.snr)\n",
    "        rate = self.input_size / self.hidden_dim\n",
    "\n",
    "        noise = torch.randn(*x.size()) * np.sqrt(1/(2 * rate * snr_lin))\n",
    "        noise_cpu = noise.detach().numpy()\n",
    "        # Convolve each sequence element by a sin function that rotates in frequency\n",
    "        # at rate given by discrete_jumps\n",
    "        res = torch.zeros(x.shape)\n",
    "        for i in range(noise_cpu.shape[0]):\n",
    "            freq = i % discrete_jumps\n",
    "            omega = L[freq]\n",
    "            t = np.linspace(0, 63, num=x.shape[2])\n",
    "            noise_tone = torch.tensor(np.sin(omega * t)).float()\n",
    "            \n",
    "            noise_convolved = np.convolve(noise_tone, noise_cpu[i, 0, :], 'same')\n",
    "            noise_convolved = torch.tensor(noise_convolved).float()\n",
    "            res[i] = noise_convolved.view(1, -1)\n",
    "        \n",
    "        x += res\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def forward(self, x, batch_idx, discrete_jumps, num_samples):\n",
    "        \n",
    "        batch_size = x.size(1)\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "#         out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.awgn(out, batch_idx, discrete_jumps, num_samples)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200.............\n",
      "Loss: 1.1195\n",
      "Acc: 0.5049\n",
      "Epoch: 2/200.............\n",
      "Loss: 0.6788\n",
      "Acc: 0.5501\n",
      "Epoch: 3/200.............\n",
      "Loss: 0.6772\n",
      "Acc: 0.5413\n",
      "Epoch: 4/200.............\n",
      "Loss: 0.6243\n",
      "Acc: 0.5705\n",
      "Epoch: 5/200.............\n",
      "Loss: 0.5253\n",
      "Acc: 0.6026\n",
      "Epoch: 6/200.............\n",
      "Loss: 0.5604\n",
      "Acc: 0.6163\n",
      "Epoch: 7/200.............\n",
      "Loss: 0.5280\n",
      "Acc: 0.6225\n",
      "Epoch: 8/200.............\n",
      "Loss: 0.4423\n",
      "Acc: 0.6785\n",
      "Epoch: 9/200.............\n",
      "Loss: 0.4335\n",
      "Acc: 0.6823\n",
      "Epoch: 10/200.............\n",
      "Loss: 0.3106\n",
      "Acc: 0.7577\n",
      "Epoch: 11/200.............\n",
      "Loss: 0.3438\n",
      "Acc: 0.7418\n",
      "Epoch: 12/200.............\n",
      "Loss: 0.2823\n",
      "Acc: 0.7748\n",
      "Epoch: 13/200.............\n",
      "Loss: 0.2895\n",
      "Acc: 0.7893\n",
      "Epoch: 14/200.............\n",
      "Loss: 0.3190\n",
      "Acc: 0.7848\n",
      "Epoch: 15/200.............\n",
      "Loss: 0.2215\n",
      "Acc: 0.8258\n",
      "Epoch: 16/200.............\n",
      "Loss: 0.2720\n",
      "Acc: 0.8149\n",
      "Epoch: 17/200.............\n",
      "Loss: 0.2542\n",
      "Acc: 0.8256\n",
      "Epoch: 18/200.............\n",
      "Loss: 0.2223\n",
      "Acc: 0.8448\n",
      "Epoch: 19/200.............\n",
      "Loss: 0.1647\n",
      "Acc: 0.8643\n",
      "Epoch: 20/200.............\n",
      "Loss: 0.1770\n",
      "Acc: 0.8684\n",
      "Epoch: 21/200.............\n",
      "Loss: 0.1724\n",
      "Acc: 0.8739\n",
      "Epoch: 22/200.............\n",
      "Loss: 0.1542\n",
      "Acc: 0.8849\n",
      "Epoch: 23/200.............\n",
      "Loss: 0.1508\n",
      "Acc: 0.8830\n",
      "Epoch: 24/200.............\n",
      "Loss: 0.1443\n",
      "Acc: 0.8859\n",
      "Epoch: 25/200.............\n",
      "Loss: 0.1658\n",
      "Acc: 0.8830\n",
      "Epoch: 26/200.............\n",
      "Loss: 0.1707\n",
      "Acc: 0.8794\n",
      "Epoch: 27/200.............\n",
      "Loss: 0.2587\n",
      "Acc: 0.8515\n",
      "Epoch: 28/200.............\n",
      "Loss: 0.1493\n",
      "Acc: 0.8888\n",
      "Epoch: 29/200.............\n",
      "Loss: 0.1847\n",
      "Acc: 0.8892\n",
      "Epoch: 30/200.............\n",
      "Loss: 0.1176\n",
      "Acc: 0.9134\n",
      "Epoch: 31/200.............\n",
      "Loss: 0.1450\n",
      "Acc: 0.9006\n",
      "Epoch: 32/200.............\n",
      "Loss: 0.1385\n",
      "Acc: 0.9067\n",
      "Epoch: 33/200.............\n",
      "Loss: 0.1380\n",
      "Acc: 0.9108\n",
      "Epoch: 34/200.............\n",
      "Loss: 0.1619\n",
      "Acc: 0.8959\n",
      "Epoch: 35/200.............\n",
      "Loss: 0.1117\n",
      "Acc: 0.9209\n",
      "Epoch: 36/200.............\n",
      "Loss: 0.0987\n",
      "Acc: 0.9261\n",
      "Epoch: 37/200.............\n",
      "Loss: 0.1297\n",
      "Acc: 0.9133\n",
      "Epoch: 38/200.............\n",
      "Loss: 0.0789\n",
      "Acc: 0.9324\n",
      "Epoch: 39/200.............\n",
      "Loss: 0.1242\n",
      "Acc: 0.9154\n",
      "Epoch: 40/200.............\n",
      "Loss: 0.0984\n",
      "Acc: 0.9299\n",
      "Epoch: 41/200.............\n",
      "Loss: 0.1353\n",
      "Acc: 0.9036\n",
      "Epoch: 42/200.............\n",
      "Loss: 0.1317\n",
      "Acc: 0.9086\n",
      "Epoch: 43/200.............\n",
      "Loss: 0.0743\n",
      "Acc: 0.9445\n",
      "Epoch: 44/200.............\n",
      "Loss: 0.1053\n",
      "Acc: 0.9316\n",
      "Epoch: 45/200.............\n",
      "Loss: 0.0642\n",
      "Acc: 0.9473\n",
      "Epoch: 46/200.............\n",
      "Loss: 0.1077\n",
      "Acc: 0.9321\n",
      "Epoch: 47/200.............\n",
      "Loss: 0.0820\n",
      "Acc: 0.9410\n",
      "Epoch: 48/200.............\n",
      "Loss: 0.1035\n",
      "Acc: 0.9396\n",
      "Epoch: 49/200.............\n",
      "Loss: 0.1113\n",
      "Acc: 0.9307\n",
      "Epoch: 50/200.............\n",
      "Loss: 0.0944\n",
      "Acc: 0.9323\n",
      "Epoch: 51/200.............\n",
      "Loss: 0.0670\n",
      "Acc: 0.9510\n",
      "Epoch: 52/200.............\n",
      "Loss: 0.1016\n",
      "Acc: 0.9296\n",
      "Epoch: 53/200.............\n",
      "Loss: 0.1186\n",
      "Acc: 0.9285\n",
      "Epoch: 54/200.............\n",
      "Loss: 0.0587\n",
      "Acc: 0.9545\n",
      "Epoch: 55/200.............\n",
      "Loss: 0.1186\n",
      "Acc: 0.9277\n",
      "Epoch: 56/200.............\n",
      "Loss: 0.1316\n",
      "Acc: 0.9251\n",
      "Epoch: 57/200.............\n",
      "Loss: 0.1041\n",
      "Acc: 0.9257\n",
      "Epoch: 58/200.............\n",
      "Loss: 0.0978\n",
      "Acc: 0.9359\n",
      "Epoch: 59/200.............\n",
      "Loss: 0.1463\n",
      "Acc: 0.9219\n",
      "Epoch: 60/200.............\n",
      "Loss: 0.0628\n",
      "Acc: 0.9528\n",
      "Epoch: 61/200.............\n",
      "Loss: 0.0804\n",
      "Acc: 0.9463\n",
      "Epoch: 62/200.............\n",
      "Loss: 0.0561\n",
      "Acc: 0.9545\n",
      "Epoch: 63/200.............\n",
      "Loss: 0.0603\n",
      "Acc: 0.9525\n",
      "Epoch: 64/200.............\n",
      "Loss: 0.0935\n",
      "Acc: 0.9400\n",
      "Epoch: 65/200.............\n",
      "Loss: 0.1228\n",
      "Acc: 0.9343\n",
      "Epoch: 66/200.............\n",
      "Loss: 0.0929\n",
      "Acc: 0.9432\n",
      "Epoch: 67/200.............\n",
      "Loss: 0.0631\n",
      "Acc: 0.9530\n",
      "Epoch: 68/200.............\n",
      "Loss: 0.0757\n",
      "Acc: 0.9521\n",
      "Epoch: 69/200.............\n",
      "Loss: 0.0759\n",
      "Acc: 0.9513\n",
      "Epoch: 70/200.............\n",
      "Loss: 0.0692\n",
      "Acc: 0.9483\n",
      "Epoch: 71/200.............\n",
      "Loss: 0.0733\n",
      "Acc: 0.9511\n",
      "Epoch: 72/200.............\n",
      "Loss: 0.0483\n",
      "Acc: 0.9634\n",
      "Epoch: 73/200.............\n",
      "Loss: 0.0774\n",
      "Acc: 0.9522\n",
      "Epoch: 74/200.............\n",
      "Loss: 0.0848\n",
      "Acc: 0.9447\n",
      "Epoch: 75/200.............\n",
      "Loss: 0.0579\n",
      "Acc: 0.9613\n",
      "Epoch: 76/200.............\n",
      "Loss: 0.0411\n",
      "Acc: 0.9665\n",
      "Epoch: 77/200.............\n",
      "Loss: 0.0976\n",
      "Acc: 0.9438\n",
      "Epoch: 78/200.............\n",
      "Loss: 0.0955\n",
      "Acc: 0.9481\n",
      "Epoch: 79/200.............\n",
      "Loss: 0.1142\n",
      "Acc: 0.9396\n",
      "Epoch: 80/200.............\n",
      "Loss: 0.0833\n",
      "Acc: 0.9540\n",
      "Epoch: 81/200.............\n",
      "Loss: 0.0656\n",
      "Acc: 0.9552\n",
      "Epoch: 82/200.............\n",
      "Loss: 0.0586\n",
      "Acc: 0.9546\n",
      "Epoch: 83/200.............\n",
      "Loss: 0.0247\n",
      "Acc: 0.9797\n",
      "Epoch: 84/200.............\n",
      "Loss: 0.1051\n",
      "Acc: 0.9432\n",
      "Epoch: 85/200.............\n",
      "Loss: 0.0465\n",
      "Acc: 0.9648\n",
      "Epoch: 86/200.............\n",
      "Loss: 0.0517\n",
      "Acc: 0.9625\n",
      "Epoch: 87/200.............\n",
      "Loss: 0.0955\n",
      "Acc: 0.9475\n",
      "Epoch: 88/200.............\n",
      "Loss: 0.0579\n",
      "Acc: 0.9576\n",
      "Epoch: 89/200.............\n",
      "Loss: 0.0510\n",
      "Acc: 0.9625\n",
      "Epoch: 90/200.............\n",
      "Loss: 0.0374\n",
      "Acc: 0.9709\n",
      "Epoch: 91/200.............\n",
      "Loss: 0.0826\n",
      "Acc: 0.9474\n",
      "Epoch: 92/200.............\n",
      "Loss: 0.0596\n",
      "Acc: 0.9606\n",
      "Epoch: 93/200.............\n",
      "Loss: 0.0657\n",
      "Acc: 0.9593\n",
      "Epoch: 94/200.............\n",
      "Loss: 0.1212\n",
      "Acc: 0.9207\n",
      "Epoch: 95/200.............\n",
      "Loss: 0.1319\n",
      "Acc: 0.9425\n",
      "Epoch: 96/200.............\n",
      "Loss: 0.0621\n",
      "Acc: 0.9563\n",
      "Epoch: 97/200.............\n",
      "Loss: 0.0320\n",
      "Acc: 0.9709\n",
      "Epoch: 98/200.............\n",
      "Loss: 0.1101\n",
      "Acc: 0.9542\n",
      "Epoch: 99/200.............\n",
      "Loss: 0.0239\n",
      "Acc: 0.9787\n",
      "Epoch: 100/200.............\n",
      "Loss: 1.1947\n",
      "Acc: 0.8029\n",
      "Epoch: 101/200.............\n",
      "Loss: 0.5727\n",
      "Acc: 0.8461\n",
      "Epoch: 102/200.............\n",
      "Loss: 0.2626\n",
      "Acc: 0.8948\n",
      "Epoch: 103/200.............\n",
      "Loss: 0.2571\n",
      "Acc: 0.8997\n",
      "Epoch: 104/200.............\n",
      "Loss: 0.1624\n",
      "Acc: 0.9220\n",
      "Epoch: 105/200.............\n",
      "Loss: 0.2342\n",
      "Acc: 0.9009\n",
      "Epoch: 106/200.............\n",
      "Loss: 0.2205\n",
      "Acc: 0.9045\n",
      "Epoch: 107/200.............\n",
      "Loss: 0.2249\n",
      "Acc: 0.8979\n",
      "Epoch: 108/200.............\n",
      "Loss: 0.1207\n",
      "Acc: 0.9367\n",
      "Epoch: 109/200.............\n",
      "Loss: 0.1931\n",
      "Acc: 0.9137\n",
      "Epoch: 110/200.............\n",
      "Loss: 0.1820\n",
      "Acc: 0.9179\n",
      "Epoch: 111/200.............\n",
      "Loss: 0.2657\n",
      "Acc: 0.8880\n",
      "Epoch: 112/200.............\n",
      "Loss: 0.1487\n",
      "Acc: 0.9231\n",
      "Epoch: 113/200.............\n",
      "Loss: 0.1784\n",
      "Acc: 0.9291\n",
      "Epoch: 114/200.............\n",
      "Loss: 0.2514\n",
      "Acc: 0.8882\n",
      "Epoch: 115/200.............\n",
      "Loss: 0.2634\n",
      "Acc: 0.8960\n",
      "Epoch: 116/200.............\n",
      "Loss: 0.1300\n",
      "Acc: 0.9203\n",
      "Epoch: 117/200.............\n",
      "Loss: 0.1644\n",
      "Acc: 0.9173\n",
      "Epoch: 118/200.............\n",
      "Loss: 0.1180\n",
      "Acc: 0.9310\n",
      "Epoch: 119/200.............\n",
      "Loss: 0.1578\n",
      "Acc: 0.9270\n",
      "Epoch: 120/200.............\n",
      "Loss: 0.1577\n",
      "Acc: 0.9172\n",
      "Epoch: 121/200.............\n",
      "Loss: 0.1519\n",
      "Acc: 0.9250\n",
      "Epoch: 122/200.............\n",
      "Loss: 0.1742\n",
      "Acc: 0.9150\n",
      "Epoch: 123/200.............\n",
      "Loss: 0.1325\n",
      "Acc: 0.9293\n",
      "Epoch: 124/200.............\n",
      "Loss: 0.1777\n",
      "Acc: 0.9093\n",
      "Epoch: 125/200.............\n",
      "Loss: 0.1515\n",
      "Acc: 0.9285\n",
      "Epoch: 126/200.............\n",
      "Loss: 0.1539\n",
      "Acc: 0.9069\n",
      "Epoch: 127/200.............\n",
      "Loss: 0.1928\n",
      "Acc: 0.9151\n",
      "Epoch: 128/200.............\n",
      "Loss: 0.1342\n",
      "Acc: 0.9249\n",
      "Epoch: 129/200.............\n",
      "Loss: 0.1259\n",
      "Acc: 0.9388\n",
      "Epoch: 130/200.............\n",
      "Loss: 0.0886\n",
      "Acc: 0.9447\n",
      "Epoch: 131/200.............\n",
      "Loss: 0.1247\n",
      "Acc: 0.9246\n",
      "Epoch: 132/200.............\n",
      "Loss: 0.0738\n",
      "Acc: 0.9508\n",
      "Epoch: 133/200.............\n",
      "Loss: 0.2501\n",
      "Acc: 0.8917\n",
      "Epoch: 134/200.............\n",
      "Loss: 0.2668\n",
      "Acc: 0.8890\n",
      "Epoch: 135/200.............\n",
      "Loss: 0.1908\n",
      "Acc: 0.9002\n",
      "Epoch: 136/200.............\n",
      "Loss: 0.1164\n",
      "Acc: 0.9207\n",
      "Epoch: 137/200.............\n",
      "Loss: 0.1317\n",
      "Acc: 0.9320\n",
      "Epoch: 138/200.............\n",
      "Loss: 0.1990\n",
      "Acc: 0.9204\n",
      "Epoch: 139/200.............\n",
      "Loss: 0.1295\n",
      "Acc: 0.9291\n",
      "Epoch: 140/200.............\n",
      "Loss: 0.1657\n",
      "Acc: 0.9300\n",
      "Epoch: 141/200.............\n",
      "Loss: 0.2022\n",
      "Acc: 0.9144\n",
      "Epoch: 142/200.............\n",
      "Loss: 0.0746\n",
      "Acc: 0.9528\n",
      "Epoch: 143/200.............\n",
      "Loss: 0.1709\n",
      "Acc: 0.9110\n",
      "Epoch: 144/200.............\n",
      "Loss: 0.0969\n",
      "Acc: 0.9346\n",
      "Epoch: 145/200.............\n",
      "Loss: 0.1859\n",
      "Acc: 0.9117\n",
      "Epoch: 146/200.............\n",
      "Loss: 0.1227\n",
      "Acc: 0.9299\n",
      "Epoch: 147/200.............\n",
      "Loss: 0.1205\n",
      "Acc: 0.9270\n",
      "Epoch: 148/200.............\n",
      "Loss: 0.1425\n",
      "Acc: 0.9264\n",
      "Epoch: 149/200.............\n",
      "Loss: 0.0901\n",
      "Acc: 0.9428\n",
      "Epoch: 150/200.............\n",
      "Loss: 0.0879\n",
      "Acc: 0.9370\n",
      "Epoch: 151/200.............\n",
      "Loss: 0.1442\n",
      "Acc: 0.9221\n",
      "Epoch: 152/200.............\n",
      "Loss: 0.1230\n",
      "Acc: 0.9373\n",
      "Epoch: 153/200.............\n",
      "Loss: 0.1233\n",
      "Acc: 0.9265\n",
      "Epoch: 154/200.............\n",
      "Loss: 0.1166\n",
      "Acc: 0.9397\n",
      "Epoch: 155/200.............\n",
      "Loss: 0.0716\n",
      "Acc: 0.9548\n",
      "Epoch: 156/200.............\n",
      "Loss: 0.1366\n",
      "Acc: 0.9371\n",
      "Epoch: 157/200.............\n",
      "Loss: 0.2351\n",
      "Acc: 0.9135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 158/200.............\n",
      "Loss: 0.1265\n",
      "Acc: 0.9270\n",
      "Epoch: 159/200.............\n",
      "Loss: 0.1632\n",
      "Acc: 0.9221\n",
      "Epoch: 160/200.............\n",
      "Loss: 0.1343\n",
      "Acc: 0.9330\n",
      "Epoch: 161/200.............\n",
      "Loss: 0.1027\n",
      "Acc: 0.9350\n",
      "Epoch: 162/200.............\n",
      "Loss: 0.0860\n",
      "Acc: 0.9407\n",
      "Epoch: 163/200.............\n",
      "Loss: 0.0996\n",
      "Acc: 0.9339\n",
      "Epoch: 164/200.............\n",
      "Loss: 0.0824\n",
      "Acc: 0.9431\n",
      "Epoch: 165/200.............\n",
      "Loss: 0.1534\n",
      "Acc: 0.9218\n",
      "Epoch: 166/200.............\n",
      "Loss: 0.1152\n",
      "Acc: 0.9363\n",
      "Epoch: 167/200.............\n",
      "Loss: 0.1577\n",
      "Acc: 0.9317\n",
      "Epoch: 168/200.............\n",
      "Loss: 0.1014\n",
      "Acc: 0.9396\n",
      "Epoch: 169/200.............\n",
      "Loss: 0.1329\n",
      "Acc: 0.9336\n",
      "Epoch: 170/200.............\n",
      "Loss: 0.1026\n",
      "Acc: 0.9370\n",
      "Epoch: 171/200.............\n",
      "Loss: 0.1655\n",
      "Acc: 0.9138\n",
      "Epoch: 172/200.............\n",
      "Loss: 0.0977\n",
      "Acc: 0.9416\n",
      "Epoch: 173/200.............\n",
      "Loss: 0.1826\n",
      "Acc: 0.9220\n"
     ]
    }
   ],
   "source": [
    "block_size = 4\n",
    "channel_use = 32\n",
    "\n",
    "n_epochs = 200\n",
    "lr = 0.01\n",
    "batch_size = 50\n",
    "num_taps = channel_use\n",
    "snr = 0\n",
    "\n",
    "discrete_jumps = 5\n",
    "num_samples = discrete_jumps*150\n",
    "\n",
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=block_size, output_size=block_size, hidden_dim=channel_use, n_layers=1, snr=snr, num_taps=num_taps)\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# Data loading\n",
    "train_data, train_labels, test_data, test_labels = generate_data(num_samples, block_size, False)\n",
    "params = {'batch_size': batch_size,\n",
    "       'shuffle': True}\n",
    "training_loader = torch.utils.data.DataLoader(Dataset(train_data, train_labels), **params)\n",
    "def accuracy(preds, labels):\n",
    "    preds = preds.squeeze()\n",
    "    labels = labels.squeeze()\n",
    "    return 1 - torch.sum(torch.abs(preds-labels)).item() / (list(preds.size())[0]*list(preds.size())[1])    \n",
    "\n",
    "def show_fft(hidden, epoch, hidden_dim):\n",
    "    L = np.linspace(0, np.pi, num=discrete_jumps)\n",
    "    sig = 0\n",
    "    for omega in L:\n",
    "        t = np.linspace(0, 63, num=hidden_dim)\n",
    "        noise_tone = torch.tensor(np.sin(omega * t)).float()\n",
    "        sig += noise_tone\n",
    "    \n",
    "    noise = sig\n",
    "    \n",
    "    hidden = hidden[0, 0, :].detach().numpy()\n",
    "    noise = noise.detach().numpy()\n",
    "    \n",
    "    hidden_zp = np.zeros(200)\n",
    "    noise_zp = np.zeros(200)\n",
    "    hidden_zp[:len(hidden)] = hidden\n",
    "    noise_zp[:len(noise)] = noise\n",
    "    H = np.fft.fft(hidden_zp) / len(hidden_zp)\n",
    "    N = np.fft.fft(noise_zp) / len(noise_zp)\n",
    "    fig = plt.figure()\n",
    "    plt.plot([np.abs(H[i]) for i in range(len(H))])\n",
    "    plt.plot([np.abs(N[i]) for i in range(len(N))])\n",
    "    plt.title('RNN (32, 4), Epoch %s' % str(epoch))\n",
    "    legend_strings = []\n",
    "    legend_strings.append('Encoded Signal')\n",
    "    legend_strings.append('Time-Varying Noise Tones, Superimposed')\n",
    "    plt.legend(legend_strings, loc = 'upper right')\n",
    "    plt.ylim([0, 0.2])\n",
    "#     plt.show()\n",
    "    plt.savefig('../pics/pics' + str(epoch).zfill(4))\n",
    "    plt.close()\n",
    "    fig.clf()\n",
    "\n",
    "# Training Run\n",
    "loss_list = []\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "#     for batch in range(len(train_data)):\n",
    "    for batch_idx, (batch, labels) in enumerate(training_loader):\n",
    "        # First dim is seq_len, which is time-series data that rnn learns from\n",
    "        # Second dim is batch size\n",
    "        # Last dim is data itself\n",
    "        batch = batch.unsqueeze(1) \n",
    "        labels = labels.unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        discrete_jumps = 5 if epoch < 100 else 7\n",
    "        output, hidden = model(batch, batch_idx, discrete_jumps, num_samples)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward() # Does backpropagation and calculates gradients\n",
    "        optimizer.step() # Updates the weights accordingly\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print('Epoch: {}/{}.............'.format(epoch, n_epochs))\n",
    "            print(\"Loss: {:.4f}\".format(loss.item()))\n",
    "            print(\"Acc: {:.4f}\".format(accuracy(torch.sigmoid(torch.round(output)), labels)))\n",
    "            show_fft(hidden, epoch, channel_use)\n",
    "#         loss_list.append(loss)\n",
    "\n",
    "#         val_output, val_hidden, val_noise = model(batch, i)\n",
    "        \n",
    "#     if epoch % 10 == 0:\n",
    "        \n",
    "            \n",
    "# plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./pics/pics0001.png', './pics/pics0002.png', './pics/pics0003.png', './pics/pics0004.png', './pics/pics0005.png', './pics/pics0006.png', './pics/pics0007.png', './pics/pics0008.png', './pics/pics0009.png', './pics/pics0010.png', './pics/pics0011.png', './pics/pics0012.png', './pics/pics0013.png', './pics/pics0014.png', './pics/pics0015.png', './pics/pics0016.png', './pics/pics0017.png', './pics/pics0018.png', './pics/pics0019.png', './pics/pics0020.png', './pics/pics0021.png', './pics/pics0022.png', './pics/pics0023.png', './pics/pics0024.png', './pics/pics0025.png', './pics/pics0026.png', './pics/pics0027.png', './pics/pics0028.png', './pics/pics0029.png', './pics/pics0030.png', './pics/pics0031.png', './pics/pics0032.png', './pics/pics0033.png', './pics/pics0034.png', './pics/pics0035.png', './pics/pics0036.png', './pics/pics0037.png', './pics/pics0038.png', './pics/pics0039.png', './pics/pics0040.png', './pics/pics0041.png', './pics/pics0042.png', './pics/pics0043.png', './pics/pics0044.png', './pics/pics0045.png', './pics/pics0046.png', './pics/pics0047.png', './pics/pics0048.png', './pics/pics0049.png', './pics/pics0050.png', './pics/pics0051.png', './pics/pics0052.png', './pics/pics0053.png', './pics/pics0054.png', './pics/pics0055.png', './pics/pics0056.png', './pics/pics0057.png', './pics/pics0058.png', './pics/pics0059.png', './pics/pics0060.png', './pics/pics0061.png', './pics/pics0062.png', './pics/pics0063.png', './pics/pics0064.png', './pics/pics0065.png', './pics/pics0066.png', './pics/pics0067.png', './pics/pics0068.png', './pics/pics0069.png', './pics/pics0070.png', './pics/pics0071.png', './pics/pics0072.png', './pics/pics0073.png', './pics/pics0074.png', './pics/pics0075.png', './pics/pics0076.png', './pics/pics0077.png', './pics/pics0078.png', './pics/pics0079.png', './pics/pics0080.png', './pics/pics0081.png', './pics/pics0082.png', './pics/pics0083.png', './pics/pics0084.png', './pics/pics0085.png', './pics/pics0086.png', './pics/pics0087.png', './pics/pics0088.png', './pics/pics0089.png', './pics/pics0090.png', './pics/pics0091.png', './pics/pics0092.png', './pics/pics0093.png', './pics/pics0094.png', './pics/pics0095.png', './pics/pics0096.png', './pics/pics0097.png', './pics/pics0098.png', './pics/pics0099.png', './pics/pics0100.png', './pics/pics0101.png', './pics/pics0102.png', './pics/pics0103.png', './pics/pics0104.png', './pics/pics0105.png', './pics/pics0106.png', './pics/pics0107.png', './pics/pics0108.png', './pics/pics0109.png', './pics/pics0110.png', './pics/pics0111.png', './pics/pics0112.png', './pics/pics0113.png', './pics/pics0114.png', './pics/pics0115.png', './pics/pics0116.png', './pics/pics0117.png', './pics/pics0118.png', './pics/pics0119.png', './pics/pics0120.png', './pics/pics0121.png', './pics/pics0122.png', './pics/pics0123.png', './pics/pics0124.png', './pics/pics0125.png', './pics/pics0126.png', './pics/pics0127.png', './pics/pics0128.png', './pics/pics0129.png', './pics/pics0130.png', './pics/pics0131.png', './pics/pics0132.png', './pics/pics0133.png', './pics/pics0134.png', './pics/pics0135.png', './pics/pics0136.png', './pics/pics0137.png', './pics/pics0138.png', './pics/pics0139.png', './pics/pics0140.png', './pics/pics0141.png', './pics/pics0142.png', './pics/pics0143.png', './pics/pics0144.png', './pics/pics0145.png', './pics/pics0146.png', './pics/pics0147.png', './pics/pics0148.png', './pics/pics0149.png', './pics/pics0150.png', './pics/pics0151.png', './pics/pics0152.png', './pics/pics0153.png', './pics/pics0154.png', './pics/pics0155.png', './pics/pics0156.png', './pics/pics0157.png', './pics/pics0158.png', './pics/pics0159.png', './pics/pics0160.png', './pics/pics0161.png', './pics/pics0162.png', './pics/pics0163.png', './pics/pics0164.png', './pics/pics0165.png', './pics/pics0166.png', './pics/pics0167.png', './pics/pics0168.png', './pics/pics0169.png', './pics/pics0170.png', './pics/pics0171.png', './pics/pics0172.png', './pics/pics0173.png', './pics/pics0174.png', './pics/pics0175.png', './pics/pics0176.png', './pics/pics0177.png', './pics/pics0178.png', './pics/pics0179.png', './pics/pics0180.png', './pics/pics0181.png', './pics/pics0182.png', './pics/pics0183.png', './pics/pics0184.png', './pics/pics0185.png', './pics/pics0186.png', './pics/pics0187.png', './pics/pics0188.png', './pics/pics0189.png', './pics/pics0190.png', './pics/pics0191.png', './pics/pics0192.png', './pics/pics0193.png', './pics/pics0194.png', './pics/pics0195.png', './pics/pics0196.png', './pics/pics0197.png', './pics/pics0198.png', './pics/pics0199.png', './pics/pics0200.png']\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import glob\n",
    "filenames = glob.glob(\"./pics/*.png\")\n",
    "filenames = sorted(filenames)\n",
    "print(filenames)\n",
    "images = []\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(filename))\n",
    "imageio.mimsave('./learn.gif', images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
